{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Course: Bayesian Filtering and Smoothing.\n",
    "* ### Exercised round: 2\n",
    "* ### Student's name: Diego Alejandro Agudelo Espa√±a."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 (Linear Bayesian Estimation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that in the linear regression model from the previous exercise round (Ex. 1.1), we set independent Gaussian priors for the parameters $\\theta_1$ and $\\theta_2$ as follows:\n",
    "$$\n",
    "\\begin{align}\n",
    "\t\\theta_1 &= \\mathcal{N}(0, \\sigma^2),\\\\\n",
    "\t\\theta_2 &= \\mathcal{N}(0, \\sigma^2),\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where the variance $\\sigma^2$ is known. The measurements $y_k$ are modeled as\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\ty_k = \\theta_1 x_k + \\theta_2 + \\varepsilon_k, \\qquad k = 1, 2, \\cdots, T,\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where the $\\varepsilon_k$ are independent Gaussian error terms with mean $0$ and variance $1$, that is, $\\varepsilon_k \\sim \\mathcal{N}(0, 1)$. The values $x_k$ are fixed and known. The posterior distribution can be now written as.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\tp(\\theta| y_1,\\cdots,y_T) \\propto \\exp \\bigg\\{ -\\frac{1}{2} \\sum_{k=1}^{T} \\begin{pmatrix} y_k-\\theta_1 x_k-\\theta_2 \\end{pmatrix}^2 \\bigg\\} \\exp \\bigg\\{ -\\frac{1}{2\\sigma^2} \\theta_1^2 \\bigg\\} \\exp \\bigg\\{ -\\frac{1}{2\\sigma^2} \\theta_2^2 \\bigg\\}.\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "The posterior distribution can be seen to be Gaussian and your task is to derive its mean and covariance.\n",
    "\n",
    "1. Write the exponent of the posterior distribution in matrix form as in Exercise 1 on round 1 (in terms of $\\mathbf{y}$, $\\mathbf{X}$, $\\mathbf{\\theta}$ and $ \\sigma^2$).\n",
    "2. Because a Gaussian distribution is always symmetric, its mean $\\mathbf{m}$ is at the maximum of the distribution. Find the posterior mean by computing the gradient of the exponent and finding where it vanishes.\n",
    "3. Find the covariance of the distribution by computing the second derivative matrix (Hessian matrix) $\\mathbf{H}$ of the exponent. The posterior covariance is then $\\mathbf{P} = -\\mathbf{H}^{-1}$ (why?).\n",
    "4. What is the resulting posterior distribution? What is the relationship with the least squares estimate in Exercise 1 on round 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ p(\\theta| y_1,\\cdots,y_T) \\propto \\exp \\bigg\\{ -\\frac{1}{2} (\\mathbf{y} - \\mathbf{X} \\mathbf{\\theta})^\\top (\\mathbf{y} - \\mathbf{X} \\mathbf{\\theta}) - \\frac{1}{2 \\sigma^2} \\mathcal{\\theta}^\\top \\mathcal{\\theta} \\bigg\\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $E(\\theta)$ be the exponent of $p(\\theta| y_1,\\cdots,y_T)$. It holds that:\n",
    "    \n",
    "$$\\begin{align}\n",
    "   E(\\theta) & = -\\frac{1}{2} (\\mathbf{y} - \\mathbf{X} \\mathbf{\\theta})^\\top (\\mathbf{y} - \\mathbf{X} \\mathbf{\\theta}) - \\frac{1}{2 \\sigma^2} \\mathcal{\\theta}^\\top \\mathcal{\\theta} \\\\\n",
    "             & = -\\frac{1}{2} \\mathcal{y}^\\top \\mathcal{y} + \\mathcal{y}^\\top \\mathbf{X} \\mathbf{\\theta}  -\\frac{1}{2} \\mathbf{\\theta}^\\top \\mathbf{X}^\\top \\mathbf{X} \\mathbf{\\theta} - \\frac{1}{2 \\sigma^2} \\mathcal{\\theta}^\\top \\mathcal{\\theta}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now taking the gradient of $E(\\mathbf{\\theta})$ we get:\n",
    "\n",
    "$$\n",
    " \\frac{\\partial E}{\\partial \\mathbf{\\theta}} = \\mathbf{X}^\\top \\mathbf{y} - \\mathbf{X}^\\top \\mathbf{X} \\mathbf{\\theta} - \\frac{1}{\\sigma^2} \\mathcal{\\theta}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to determine the mean ($\\mathbf{m}$) we should make the gradient equal to $0$ and solve for $\\mathbf{\\theta}$.\n",
    "\n",
    "$$ \n",
    "\\mathbf{X}^\\top \\mathbf{X} \\mathbf{\\theta} + \\frac{1}{\\sigma^2} \\mathcal{\\theta} = \\mathbf{X}^\\top \\mathbf{y} \n",
    "$$\n",
    "\n",
    "$$\n",
    "(\\mathbf{X}^\\top \\mathbf{X} + \\frac{1}{\\sigma^2} \\mathbf{I}) \\mathcal{\\theta} = \\mathbf{X}^\\top \\mathbf{y}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal{\\theta} = (\\mathbf{X}^\\top \\mathbf{X} + \\frac{1}{\\sigma^2} \\mathbf{I})^{-1} \\mathbf{X}^\\top \\mathbf{y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore the mean of the posterior distribution corresponds to $ \\mathbf{m} = (\\mathbf{X}^\\top \\mathbf{X} + \\frac{1}{\\sigma^2} \\mathbf{I})^{-1} \\mathbf{X}^\\top \\mathbf{y}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find out the Hessian matrix ($\\mathbf{H}$) of the posterior exponent it is necessary to compute the second derivative of $E(\\mathbf{\\theta})$.\n",
    "\n",
    "$$\n",
    "\\mathbf{H} = \\frac{\\partial^2 E}{\\partial^2 \\mathbf{\\theta}} = - \\bigg( \\mathbf{X}^\\top \\mathbf{X}  + \\frac{1}{\\sigma^2} \\mathbf{I} \\bigg)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior covariance is then:\n",
    "\n",
    "$$\n",
    "\\mathbf{P} = -\\mathbf{H}^{-1} = \\bigg( \\mathbf{X}^\\top \\mathbf{X}  + \\frac{1}{\\sigma^2} \\mathbf{I} \\bigg)^{-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see why $\\mathbf{P} = -\\mathbf{H}^{-1}$ (i.e the covariance matrix is equal to the negative of the inverse Hessian), recall that the exponent of a general multivariate Gaussian distribution has the following form.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    " A & =  -\\frac{1}{2} (\\mathbf{x} - \\mathbf{\\mu})^\\top \\Sigma^{-1} (\\mathbf{x} - \\mathbf{\\mu}) \\\\\n",
    "   & =  -\\frac{1}{2} \\mathbf{x}^\\top \\Sigma^{-1} \\mathbf{x}  + \\mathbf{\\mu}^\\top \\Sigma^{-1} \\mathbf{x} - \\frac{1}{2} \\mathbf{\\mu}^\\top \\Sigma^{-1} \\mathbf{\\mu}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $\\Sigma$ and $\\mu$ represent the covariance matrix and the mean respectively. It turns out that the second derivate of $A$ with respecto to $\\mathbf{x}$ is:\n",
    "\n",
    "$$ \\frac{\\partial^2 A}{\\partial^2 x} = \\mathbf{H} = - \\Sigma^{-1}$$\n",
    "\n",
    "$$ - \\mathbf{H}^{-1} = \\Sigma $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we replace $\\Sigma$ for $P$ (i.e the symbol we were using for denoting the covariance matrix) we get the expected result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting all together we get the following expresion for the posterior:\n",
    "\n",
    "$$\n",
    "p(\\theta| y_1,\\cdots,y_T) = \\mathcal{N} \\left( \\mathbf{P} \\mathbf{X}^\\top \\mathbf{y}, \\mathbf{P} \\right)\n",
    "$$\n",
    "\n",
    "with\n",
    "\n",
    "$$\n",
    "\\mathbf{P} = \\bigg( \\mathbf{X}^\\top \\mathbf{X}  + \\frac{1}{\\sigma^2} \\mathbf{I} \\bigg)^{-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the least squares solution we got from exercise 1 on round 1 was:\n",
    "\n",
    "$$ \\mathbf{\\theta} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X} y $$\n",
    "\n",
    "This solution is equal to the mean of the posterior defined above **except** for the term $\\frac{1}{\\sigma^2} \\mathbf{I}$. This term appeared as a result of assuming a Gaussian prior over the parameters $\\mathbf{\\theta}$ and plays a role of regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 (Linear Regression with the Kalman Filter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** TODO ** Write the problem statement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expressions for $\\mathbf{m}_{k - 1}$ and $\\mathbf{P}_{k - 1}$ assuming that were batch processed as in Exercise 1 are the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{m}_{k - 1} = (\\mathbf{X}_{k - 1}^\\top \\mathbf{X}_{k-1} + \\frac{1}{\\sigma^2} \\mathbf{I})^{-1} \\mathbf{X}_{k-1}^\\top \\mathbf{y}_{k-1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{P}_{k - 1} = \\bigg( \\mathbf{X}_{k - 1}^\\top \\mathbf{X}_{k - 1}  + \\frac{1}{\\sigma^2} \\mathbf{I} \\bigg)^{-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And\n",
    "\n",
    "$$\n",
    "\\mathbf{m}_{k} = (\\mathbf{X}_{k}^\\top \\mathbf{X}_{k} + \\frac{1}{\\sigma^2} \\mathbf{I})^{-1} \\mathbf{X}_{k}^\\top \\mathbf{y}_{k}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{P}_{k} = \\bigg( \\mathbf{X}_{k}^\\top \\mathbf{X}_{k}  + \\frac{1}{\\sigma^2} \\mathbf{I} \\bigg)^{-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore the posterior means can be expressed in form:\n",
    "\n",
    "$$\n",
    "\\mathbf{m}_{k - 1} = \\mathbf{P}_{k-1} \\mathbf{X}_{k-1}^\\top \\mathbf{y}_{k-1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{m}_{k} = \\mathbf{P}_{k} \\mathbf{X}_{k}^\\top \\mathbf{y}_{k}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rewriting the expressions $\\mathbf{X}_k^\\top \\mathbf{X}_k$ and $\\mathbf{X}_k^\\top y_k$ in terms of $\\mathbf{X}_{k-1}$, $\\mathbf{y}_{k-1}$, $\\mathbf{H}_k$ and $y_k$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all recall the way $\\mathbf{X}_k$ is defined:\n",
    "\n",
    "$$\n",
    "\\mathbf{X}_k = \\begin{bmatrix}\n",
    "    x_1 & 1 \\\\ \n",
    "    x_2 & 1 \\\\ \n",
    "    \\vdots & \\vdots \\\\\n",
    "    x_k & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the definition of $\\mathbf{H}_k$ we can rewrite $\\mathbf{X}_k$ as:\n",
    "\n",
    "$$\n",
    "\\mathbf{X}_k = \\begin{bmatrix}\n",
    "    \\mathbf{H}_1 \\\\ \n",
    "    \\mathbf{H}_2 \\\\ \n",
    "    \\vdots  \\\\\n",
    "    \\mathbf{H}_k \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore it can be shown that:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "   \\mathbf{X}_k^\\top \\mathbf{X}_k & = \\sum_{i = 1}^k \\mathbf{H}_i^\\top \\mathbf{H}_i \\\\\n",
    "                                  & = \\sum_{i = 1}^{k-1} \\mathbf{H}_i^\\top \\mathbf{H}_i + \\mathbf{H}_k^\\top \\mathbf{H}_k \\\\\n",
    "                                  & = \\mathbf{X}_{k-1}^\\top \\mathbf{X}_{k-1} + \\mathbf{H}_k^\\top \\mathbf{H}_k\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A similar result is derived for $\\mathbf{X}_k^\\top y_k$.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "   \\mathbf{X}_k^\\top \\mathbf{y}_k & = \\sum_{i = 1}^k \\mathbf{H}_i^\\top y_i \\\\\n",
    "                                  & = \\sum_{i = 1}^{k-1} \\mathbf{H}_i^\\top y_i + \\mathbf{H}_k^\\top y_k \\\\\n",
    "                                  & = \\mathbf{X}_{k-1}^\\top \\mathbf{y}_{k-1} + \\mathbf{H}_k^\\top y_k\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substituting these expressions in the forms of $\\mathbf{m}_{k}$ and $\\mathbf{P}_{k}$ in part 1 of this exercise the following is found:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{m}_{k} & = \\mathbf{P}_{k} \\mathbf{X}_{k}^\\top \\mathbf{y}_{k} \\\\\n",
    "               & = \\mathbf{P}_{k} (\\mathbf{X}_{k-1}^\\top \\mathbf{y}_{k-1} + \\mathbf{H}_k^\\top y_k)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "And\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{P}_{k} & = \\bigg( \\mathbf{X}_{k}^\\top \\mathbf{X}_{k}  + \\frac{1}{\\sigma^2} \\mathbf{I} \\bigg)^{-1} \\\\\n",
    "               & = \\bigg( \\mathbf{X}_{k-1}^\\top \\mathbf{X}_{k-1} + \\frac{1}{\\sigma^2} \\mathbf{I} + \\mathbf{H}_k^\\top \\mathbf{H}_k \\bigg)^{-1} \\\\\n",
    "               & =  \\bigg( \\mathbf{P}_{k-1}^{-1} + \\mathbf{H}_k^\\top \\mathbf{H}_k \\bigg)^{-1}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kalman filter equation for $\\mathbf{P}_{k - 1}$ in the particular model of this round is :\n",
    "\n",
    "$$ \\mathbf{P}_{k} = \\mathbf{P}_{k-1} - \\mathbf{K}_k S_k \\mathbf{K}_k^{\\top}$$\n",
    "\n",
    "Taking into account that $\\mathbf{K}_k = \\mathbf{P}_{k-1} \\mathbf{H}_k^\\top S_k^{-1}$, $S_k = \\mathbf{H}_k \\mathbf{P}_{k-1} \\mathbf{H}_k^{\\top} + 1$ and expanding we have:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{P}_{k} & = \\mathbf{P}_{k-1} - \\mathbf{P}_{k-1} \\mathbf{H}_k^\\top S_k^{-1} S_k (\\mathbf{P}_{k-1} \\mathbf{H}_k^\\top S_k^{-1})^{\\top} \\\\\n",
    "                   & = \\mathbf{P}_{k-1} - \\mathbf{P}_{k-1} \\mathbf{H}_k^\\top S_k^{-1} S_k (\\mathbf{P}_{k-1} \\mathbf{H}_k^\\top S_k^{-1})^{\\top} \\\\\n",
    "                   & = \\mathbf{P}_{k-1} - \\mathbf{P}_{k-1} \\mathbf{H}_k^\\top (\\mathbf{P}_{k-1} \\mathbf{H}_k^\\top S_k^{-1})^{\\top} \\\\\n",
    "                   & = \\mathbf{P}_{k-1} - \\mathbf{P}_{k-1} \\mathbf{H}_k^\\top (\\mathbf{P}_{k-1} \\mathbf{H}_k^\\top S_k^{-1})^{\\top} \\\\\n",
    "                   & = \\mathbf{P}_{k-1} - \\mathbf{P}_{k-1} \\mathbf{H}_k^\\top  S_k^{-1} \\mathbf{H}_k \\mathbf{P}_{k-1} \\\\\n",
    "                   & = \\mathbf{P}_{k-1} - \\mathbf{P}_{k-1} \\mathbf{H}_k^\\top  ( \\mathbf{H}_k \\mathbf{P}_{k-1} \\mathbf{H}_k^{\\top} + 1)^{-1} \\mathbf{H}_k \\mathbf{P}_{k-1}\n",
    "\\end{align}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now using the matrix inversion lemma we can write $\\mathbf{P}_{k}$ in a more compact way.\n",
    "\n",
    "$$\n",
    "\\mathbf{P}_{k} = ( \\mathbf{P}_{k-1}^{-1} + \\mathbf{H}_k^\\top \\mathbf{H}_k )^{-1}\n",
    "$$\n",
    "\n",
    "Which is the same expression we got in part 2 and it is in turn equivalent to the derivation of $\\mathbf{P}_{k}$ in part 1 using batch regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking into account that the *Kalman Gain* equation can be rewritten in an equivalent way as $\\mathbf{K}_k = \\mathbf{P}_{k} \\mathbf{H}_k^\\top $, we can expand the Kalman filter equation for $\\mathbf{m}_k$ as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{m}_k &= \\mathbf{m}_{k-1} + \\mathbf{K}_{k} (y_k - \\mathbf{H}_k \\mathbf{m}_{k-1}) \\\\\n",
    "             &= \\mathbf{m}_{k-1} + \\mathbf{P}_{k} \\mathbf{H}_k^\\top y_k - \\mathbf{P}_{k} \\mathbf{H}_k^\\top \\mathbf{H}_k \\mathbf{m}_{k-1} \\\\\n",
    "             &= (\\mathbf{I} - \\mathbf{P}_{k} \\mathbf{H}_k^\\top \\mathbf{H}_k) \\mathbf{m}_{k-1} + \\mathbf{P}_{k} \\mathbf{H}_k^\\top y_k \\\\\n",
    "\\end{align} \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now using the fact that $\\mathbf{P}_{k} \\mathbf{P}_{k-1}^{-1} = ( \\mathbf{I} -  \\mathbf{P}_{k} \\mathbf{H}_k^\\top \\mathbf{H}_k )^{-1}$ we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{m}_k & = \\mathbf{P}_{k} \\mathbf{P}_{k-1}^{-1} \\mathbf{m}_{k-1} + \\mathbf{P}_{k} \\mathbf{H}_k^\\top y_k \\\\\n",
    "             & = \\mathbf{P}_{k} \\mathbf{P}_{k-1}^{-1} \\mathbf{P}_{k-1} \\mathbf{X}_{k-1}^\\top \\mathbf{y}_{k-1} + \\mathbf{P}_{k} \\mathbf{H}_k^\\top y_k \\\\\n",
    "             & = \\mathbf{P}_{k} \\mathbf{X}_{k-1}^\\top \\mathbf{y}_{k-1} + \\mathbf{P}_{k} \\mathbf{H}_k^\\top y_k \\\\\n",
    "             & = \\mathbf{P}_{k} (\\mathbf{X}_{k-1}^\\top \\mathbf{y}_{k-1} +  \\mathbf{H}_k^\\top) y_k \\\\\n",
    "\\end{align} \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is the same expression we got in part 2 for $\\mathbf{m_k}$ and it is in turn equivalent to the derivation of $\\mathbf{P}_{k}$ in part 1 using batch regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the Kalman Filter gives estimations for the mean ($\\mathbf{m}_{k-1}$) and covariance ($\\mathbf{P}_{k-1}$) at the time step $k-1$ equivalent to the values given by linear batch regression of part 1 at time step $k-1$ (induction hypothesis), we have showed through parts 3 and 4 of this exercise that we are able to compute estimations for the mean ($\\mathbf{m}_{k}$) and covariance ($\\mathbf{P}_{k}$) for the time step $k$ which are equal to the ones given by linear batch regression at time step $k$. This implies that the Kalman filter and the linear batch regressor give the same results for mean and covariance at any time step $T$ under the model described in this exercise."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
